{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do correls over time without for loops:\n",
    "# 1.) https://cablab.readthedocs.io/en/latest/dat_python.html - corrcf, but how to get p-val?\n",
    "# same as above: https://github.com/esa-esdl/esdl-shared/blob/master/notebooks/Python/Python_DAT.ipynb\n",
    "# 2.) http://xarray.pydata.org/en/stable/dask.html\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import bottleneck\n",
    "\n",
    "def covariance_gufunc(x, y):\n",
    "    return ((x - x.mean(axis=-1, keepdims=True))\n",
    "            * (y - y.mean(axis=-1, keepdims=True))).mean(axis=-1)\n",
    "\n",
    "def pearson_correlation_gufunc(x, y):\n",
    "    return covariance_gufunc(x, y) / (x.std(axis=-1) * y.std(axis=-1))\n",
    "\n",
    "def spearman_correlation_gufunc(x, y):\n",
    "    x_ranks = bottleneck.rankdata(x, axis=-1)\n",
    "    y_ranks = bottleneck.rankdata(y, axis=-1)\n",
    "    return pearson_correlation_gufunc(x_ranks, y_ranks)\n",
    "\n",
    "def spearman_correlation(x, y, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        spearman_correlation_gufunc, x, y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "In [19]: rs = np.random.RandomState(0)\n",
    "\n",
    "In [20]: array1 = xr.DataArray(rs.randn(1000, 100000), dims=['place', 'time'])  # 800MB\n",
    "\n",
    "In [21]: array2 = array1 + 0.5 * rs.randn(1000, 100000)\n",
    "\n",
    "# using one core, on NumPy arrays\n",
    "In [22]: %time _ = spearman_correlation(array1, array2, 'time')\n",
    "CPU times: user 21.6 s, sys: 2.84 s, total: 24.5 s\n",
    "Wall time: 24.9 s\n",
    "\n",
    "In [23]: chunked1 = array1.chunk({'place': 10})\n",
    "\n",
    "In [24]: chunked2 = array2.chunk({'place': 10})\n",
    "\n",
    "# using all my laptop's cores, with Dask\n",
    "In [25]: r = spearman_correlation(chunked1, chunked2, 'time').compute()\n",
    "\n",
    "In [26]: %time _ = r.compute()\n",
    "CPU times: user 30.9 s, sys: 1.74 s, total: 32.6 s\n",
    "Wall time: 4.59 s\n",
    "\n",
    "# OR THIS: https://examples.dask.org/xarray.html\n",
    "corr = spearman_correlation(da.chunk({'time': -1}),\n",
    "                            da_smooth.chunk({'time': -1}),\n",
    "                            'time')\n",
    "corr\n",
    "\n",
    "# 3.) For loop methodology: https://stackoverflow.com/questions/45863969/python-how-to-find-regression-equation-of-multiple-3d-lat-lon-time-value-data \n",
    "for t in range(varA1['time'].size) :\n",
    "    for la in range(varA1['lat'].size) :\n",
    "        for lo in range(varA1['lon'].size) :\n",
    "            x = varA1.values[t,la,lo]\n",
    "            y = varB1.values[t,la,lo]\n",
    "            plt.scatter(x,y)\n",
    "for t in range(varA2['time'].size) :\n",
    "    for la in range(varA2['lat'].size) :\n",
    "        for lo in range(varA2['lon'].size) :\n",
    "            x = varA2.values[t,la,lo]\n",
    "            y = varB2.values[t,la,lo]\n",
    "            plt.scatter(x,y)\n",
    "... \n",
    "plt.show()\n",
    "# OR THIS: https://stackoverflow.com/questions/38960903/applying-numpy-polyfit-to-xarray-dataset \n",
    "ds_LR = ds.TMP_P0_L103_GST0 * 0 -9999 # Quick way to make dataarray with -9999 values but with correct dims/coords\n",
    "for cts in np.arange(0,len(ds_UA.time)):\n",
    "        for cx in ds_UA.xgrid_0.values:\n",
    "                for cy in ds_UA.ygrid_0.values:\n",
    "                        x_temp = ds_UA.Temperature[:,cts,cy,cx] # Grab the vertical profile of air temperature\n",
    "                        y_hgt  = ds_UA.Height[:,cts,cy,cx] # Grab the vertical heights of air temperature values\n",
    "                        s      = np.polyfit(y_hgt,x_temp,1) # Fit a line to the data\n",
    "                        ds_LR[cts,cy,cx].values = s[0] # Grab the slope (first element)\n",
    "                        \n",
    "#4.) THIS MAY BE THE ONE: https://stackoverflow.com/questions/52094320/with-xarray-how-to-parallelize-1d-operations-on-a-multidimensional-dataset\n",
    "It is possible to apply scipy.stats.linregress (and other non-ufuncs) to the xarray Dataset using apply_ufunc() by passing vectorize=True like so:\n",
    "\n",
    "# return a tuple of DataArrays\n",
    "res = xr.apply_ufunc(scipy.stats.linregress, ds[x], ds[y],\n",
    "        input_core_dims=[['year'], ['year']],\n",
    "        output_core_dims=[[], [], [], [], []],\n",
    "        vectorize=True)\n",
    "# add the data to the existing dataset\n",
    "for arr_name, arr in zip(array_names, res):\n",
    "    ds[arr_name] = arr\n",
    "Although still serial, apply_ufunc is around 36x faster than the loop implementation in this specific case.\n",
    "\n",
    "However the parallelization with dask is still not implemented with multiple output like the one from scipy.stats.linregress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Use apply_ufunc to eliminate need for for loops\n",
    "# --> works great! BUT no good way to get rid of nans and scipy pearsonr will not take nans...\n",
    "betnow = bet_cp_tot[400:405,10:12,10:12] # all non-nan\n",
    "skjnow = skj_cp_tot[400:405,10:12,10:12] # all non-nan\n",
    "valid_values = betnow.notnull() & skjnow.notnull()\n",
    "# the below two lines don't work, see here:\n",
    "# https://stackoverflow.com/questions/52553925/python-xarray-remove-coordinates-with-all-missing-variables?rq=1\n",
    "test = betnow.where(valid_values, drop=True) # this doesn't work\n",
    "test1 = skjnow.where(valid_values, drop=True) # this doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<xarray.DataArray (lat: 2, lon: 2)>\n",
       " array([[0.74096692, 0.76881806],\n",
       "        [0.45208595, 0.89674582]])\n",
       " Coordinates:\n",
       "   * lon      (lon) float32 162.5 167.5\n",
       "   * lat      (lat) float32 -2.5 2.5, <xarray.DataArray (lat: 2, lon: 2)>\n",
       " array([[0.15195936, 0.12870626],\n",
       "        [0.44464372, 0.03920595]])\n",
       " Coordinates:\n",
       "   * lon      (lon) float32 162.5 167.5\n",
       "   * lat      (lat) float32 -2.5 2.5)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testoutput = xr.apply_ufunc(stats.pearsonr, test1, test, input_core_dims=[['time'],['time']], output_core_dims=[[],[]],vectorize=True)\n",
    "testoutput\n",
    "# --> works great and matches #s from for loop method, but needs to have NO NANS!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[testoutput1cc, testoutput1pval]=gettempccmap_loop(test1,test,'bvss_cp_tot')\n",
    "testoutput1cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput1pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "testoutput = xr.apply_ufunc(stats.pearsonr, test1, test, input_core_dims=[['time'],['time']], output_core_dims=[[],[]],vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "[testoutput1cc, testoutput1pval]=gettempccmap_loop(test1,test,'bvss_cp_tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Convert to df to remove nans, then compute corr coeffs in df:\n",
    "# --> not great method b/c can't get back to DataArray easily and can't compute p-vals easily\n",
    "dsnow = xr.merge([skj_cp_tot, bet_cp_tot])\n",
    "dfnow = dsnow.to_dataframe()\n",
    "dfnownn = dfnow.dropna(how='any')\n",
    "dfnownn.head\n",
    "# following line doesn't work\n",
    "#stats.pearsonr(dfnownn.groupby(level=[0,1])['skj_cp_tot'],dfnownn.groupby(level=[0,1])['bet_cp_tot'])\n",
    "dfnownn.groupby(level=[0,1]).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - What if we get rid of nans in the df and then convert back to xr Dataset?\n",
    "# --> doesn't work, just gives nans back to have uniform grid\n",
    "dsnownn = dfnownn.to_xarray()\n",
    "dsnownn # --> gives nans back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - What about getting indices that are non-null and then selecting those indices?\n",
    "valid_values = betnow.notnull() & skjnow.notnull()\n",
    "vv = valid_values.sum(dim='time')>2\n",
    "vv_stacked = vv.stack(x=['lat','lon'])\n",
    "vvnow = vv_stacked[vv_stacked]\n",
    "skj_cp_tot.sel(lon=vvnow.lon.values[0], lat=vvnow.lat.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - What about getting indices that are non-null and then selecting those indices?\n",
    "# --> but how do I get the spatial structure back and correl over time dimension?\n",
    "valid_values = betnow.notnull() & skjnow.notnull()\n",
    "vv = valid_values.sum(dim='time')>2\n",
    "vv_stacked = vv.stack(x=['lat','lon'])\n",
    "vvnow = vv_stacked[vv_stacked]\n",
    "skj_cp_tot.sel(lon=vvnow.lon.values[0], lat=vvnow.lat.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_values = betnow.notnull() & skjnow.notnull()\n",
    "vv_stacked = valid_values.stack(x=['lat','lon','time'])\n",
    "vvnow = vv_stacked[vv_stacked]\n",
    "skj_cp_tot.sel(lon=vvnow.lon.values[0],lat=vvnow.lat.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skj_cp_tot_stacked = skj_cp_tot.stack(x=['lat','lon','time'])\n",
    "skj_cp_tot_stacked[vv_stacked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4 = skj_cp_tot.sel(time = vvnow.time.values, lat = vvnow.lat.values, lon=vvnow.lon.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
