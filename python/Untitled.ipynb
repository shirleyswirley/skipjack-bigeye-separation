{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do correls over time without for loops:\n",
    "# 1.) https://cablab.readthedocs.io/en/latest/dat_python.html - corrcf, but how to get p-val?\n",
    "# same as above: https://github.com/esa-esdl/esdl-shared/blob/master/notebooks/Python/Python_DAT.ipynb\n",
    "# 2.) http://xarray.pydata.org/en/stable/dask.html\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import bottleneck\n",
    "\n",
    "def covariance_gufunc(x, y):\n",
    "    return ((x - x.mean(axis=-1, keepdims=True))\n",
    "            * (y - y.mean(axis=-1, keepdims=True))).mean(axis=-1)\n",
    "\n",
    "def pearson_correlation_gufunc(x, y):\n",
    "    return covariance_gufunc(x, y) / (x.std(axis=-1) * y.std(axis=-1))\n",
    "\n",
    "def spearman_correlation_gufunc(x, y):\n",
    "    x_ranks = bottleneck.rankdata(x, axis=-1)\n",
    "    y_ranks = bottleneck.rankdata(y, axis=-1)\n",
    "    return pearson_correlation_gufunc(x_ranks, y_ranks)\n",
    "\n",
    "def spearman_correlation(x, y, dim):\n",
    "    return xr.apply_ufunc(\n",
    "        spearman_correlation_gufunc, x, y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "In [19]: rs = np.random.RandomState(0)\n",
    "\n",
    "In [20]: array1 = xr.DataArray(rs.randn(1000, 100000), dims=['place', 'time'])  # 800MB\n",
    "\n",
    "In [21]: array2 = array1 + 0.5 * rs.randn(1000, 100000)\n",
    "\n",
    "# using one core, on NumPy arrays\n",
    "In [22]: %time _ = spearman_correlation(array1, array2, 'time')\n",
    "CPU times: user 21.6 s, sys: 2.84 s, total: 24.5 s\n",
    "Wall time: 24.9 s\n",
    "\n",
    "In [23]: chunked1 = array1.chunk({'place': 10})\n",
    "\n",
    "In [24]: chunked2 = array2.chunk({'place': 10})\n",
    "\n",
    "# using all my laptop's cores, with Dask\n",
    "In [25]: r = spearman_correlation(chunked1, chunked2, 'time').compute()\n",
    "\n",
    "In [26]: %time _ = r.compute()\n",
    "CPU times: user 30.9 s, sys: 1.74 s, total: 32.6 s\n",
    "Wall time: 4.59 s\n",
    "\n",
    "# OR THIS: https://examples.dask.org/xarray.html\n",
    "corr = spearman_correlation(da.chunk({'time': -1}),\n",
    "                            da_smooth.chunk({'time': -1}),\n",
    "                            'time')\n",
    "corr\n",
    "\n",
    "# 3.) For loop methodology: https://stackoverflow.com/questions/45863969/python-how-to-find-regression-equation-of-multiple-3d-lat-lon-time-value-data \n",
    "for t in range(varA1['time'].size) :\n",
    "    for la in range(varA1['lat'].size) :\n",
    "        for lo in range(varA1['lon'].size) :\n",
    "            x = varA1.values[t,la,lo]\n",
    "            y = varB1.values[t,la,lo]\n",
    "            plt.scatter(x,y)\n",
    "for t in range(varA2['time'].size) :\n",
    "    for la in range(varA2['lat'].size) :\n",
    "        for lo in range(varA2['lon'].size) :\n",
    "            x = varA2.values[t,la,lo]\n",
    "            y = varB2.values[t,la,lo]\n",
    "            plt.scatter(x,y)\n",
    "... \n",
    "plt.show()\n",
    "# OR THIS: https://stackoverflow.com/questions/38960903/applying-numpy-polyfit-to-xarray-dataset \n",
    "ds_LR = ds.TMP_P0_L103_GST0 * 0 -9999 # Quick way to make dataarray with -9999 values but with correct dims/coords\n",
    "for cts in np.arange(0,len(ds_UA.time)):\n",
    "        for cx in ds_UA.xgrid_0.values:\n",
    "                for cy in ds_UA.ygrid_0.values:\n",
    "                        x_temp = ds_UA.Temperature[:,cts,cy,cx] # Grab the vertical profile of air temperature\n",
    "                        y_hgt  = ds_UA.Height[:,cts,cy,cx] # Grab the vertical heights of air temperature values\n",
    "                        s      = np.polyfit(y_hgt,x_temp,1) # Fit a line to the data\n",
    "                        ds_LR[cts,cy,cx].values = s[0] # Grab the slope (first element)\n",
    "                        \n",
    "#4.) THIS MAY BE THE ONE: https://stackoverflow.com/questions/52094320/with-xarray-how-to-parallelize-1d-operations-on-a-multidimensional-dataset\n",
    "It is possible to apply scipy.stats.linregress (and other non-ufuncs) to the xarray Dataset using apply_ufunc() by passing vectorize=True like so:\n",
    "\n",
    "# return a tuple of DataArrays\n",
    "res = xr.apply_ufunc(scipy.stats.linregress, ds[x], ds[y],\n",
    "        input_core_dims=[['year'], ['year']],\n",
    "        output_core_dims=[[], [], [], [], []],\n",
    "        vectorize=True)\n",
    "# add the data to the existing dataset\n",
    "for arr_name, arr in zip(array_names, res):\n",
    "    ds[arr_name] = arr\n",
    "Although still serial, apply_ufunc is around 36x faster than the loop implementation in this specific case.\n",
    "\n",
    "However the parallelization with dask is still not implemented with multiple output like the one from scipy.stats.linregress:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
